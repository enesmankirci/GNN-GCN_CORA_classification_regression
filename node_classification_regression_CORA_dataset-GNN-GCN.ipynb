{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddf2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import optuna\n",
    "import torch_geometric.utils\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e94497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    In this project, I am building a GCN model that performs both node-level classification and regression on the Cora\\n    citation dataset. It includes data preprocessing like randomly removing nodes and features, model definition,\\n    training and evaluation with hyperparameter tuning.\\n    \\n    The hyperparameters tuned are:\\n    -'hidden_channels'\\n    -'learning_rate'\\n    -'weight_decay'\\n    * Number of epochs can also be tuned, with Cross-validation.\\n    \\n    *** I hope to make additions and changes to this notebook later on to use it on my blog -linksomeneurons.com- while\\n    writing about implementation of neural networks or specifically GNN-GCN-GAN.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    In this project, I am building a GCN model that performs both node-level classification and regression on the Cora\n",
    "    citation dataset. It includes data preprocessing like randomly removing nodes and features, model definition,\n",
    "    training and evaluation with hyperparameter tuning.\n",
    "    \n",
    "    The hyperparameters tuned are:\n",
    "    -'hidden_channels'\n",
    "    -'learning_rate'\n",
    "    -'weight_decay'\n",
    "    * Number of epochs can also be tuned, with Cross-validation.\n",
    "    \n",
    "    Test Accuracy value: 0.8291 \n",
    "    Test MSE value: 0.0718\n",
    "    -The test accuracy value of 0.80 is good. The model can learn to classify nodes into their respective classes.\n",
    "    -MSE score is relatively good. For the regression task, a lower MSE score would be desired.\n",
    "    \n",
    "    *** I hope to make additions and changes to this notebook later on to use it on my blog -linksomeneurons.com- while\n",
    "    writing about implementation of neural networks or specifically GNN-GCN-GAN.\n",
    "    \n",
    "    In case of errors or improvements, do not hesitate to contact me via my website.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0102e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e38322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly removing some data points to perform node-level regression later, keeping the 80%\n",
    "num_nodes = data.num_nodes\n",
    "keep_mask = torch.rand(num_nodes) < 0.8\n",
    "data.x = data.x[keep_mask]\n",
    "data.y = data.y[keep_mask]\n",
    "data.edge_index, _ = torch_geometric.utils.subgraph(keep_mask, data.edge_index, None, relabel_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b642525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting a subset of features (e.g., keep 50% of the features)\n",
    "num_features = data.num_features\n",
    "keep_features = np.random.choice(num_features, size=num_features//2, replace=False)\n",
    "data.x = data.x[:, keep_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf2a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning a synthetic continuous target for node-level prediction\n",
    "continuous_target = torch.rand(data.num_nodes)\n",
    "\n",
    "#Combining classification and regression targets\n",
    "data.y = torch.stack([data.y.float(), continuous_target], dim=1)\n",
    "\n",
    "#Data split\n",
    "num_nodes = data.num_nodes\n",
    "train_mask, test_mask = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)\n",
    "train_mask, val_mask = train_test_split(train_mask, test_size=0.25, random_state=42)\n",
    "\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_mask] = True\n",
    "data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.val_mask[val_mask] = True\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask[test_mask] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d9683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The GCN model for both classification and regression\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.classifier = torch.nn.Linear(hidden_channels, num_classes)\n",
    "        self.regressor = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        class_output = F.log_softmax(self.classifier(x), dim=1)\n",
    "        reg_output = self.regressor(x).squeeze(1)\n",
    "        return class_output, reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28119c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    class_out, reg_out = model(data.x, data.edge_index)\n",
    "    class_loss = F.nll_loss(class_out[data.train_mask], data.y[data.train_mask, 0].long())\n",
    "    reg_loss = F.mse_loss(reg_out[data.train_mask], data.y[data.train_mask, 1])\n",
    "    loss = class_loss + reg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        class_out, reg_out = model(data.x, data.edge_index)\n",
    "        class_pred = class_out[mask].argmax(dim=1)\n",
    "        class_correct = (class_pred == data.y[mask, 0].long()).sum()\n",
    "        class_acc = int(class_correct) / int(mask.sum())\n",
    "        reg_mse = F.mse_loss(reg_out[mask], data.y[mask, 1])\n",
    "    return class_acc, reg_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f98fe6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    #Hyperparameters to optimize\n",
    "    hidden_channels = trial.suggest_int('hidden_channels', 8, 64)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    model = GCN(data.num_features, dataset.num_classes, hidden_channels)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(200):\n",
    "        loss = train(model, optimizer, data)\n",
    "        val_acc, val_mse = evaluate(model, data, data.val_mask)\n",
    "        \n",
    "        trial.report(val_acc, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94576b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-29 14:20:55,912] A new study created in memory with name: no-name-39cf2c3b-2e39-4f1b-9bb8-40aac8846d84\n",
      "[I 2024-08-29 14:20:58,830] Trial 0 finished with value: 0.8452655889145496 and parameters: {'hidden_channels': 33, 'learning_rate': 0.0019083604310603539, 'weight_decay': 4.353289752957122e-05}. Best is trial 0 with value: 0.8452655889145496.\n",
      "[I 2024-08-29 14:21:01,383] Trial 1 finished with value: 0.8475750577367206 and parameters: {'hidden_channels': 24, 'learning_rate': 0.0017478779406384542, 'weight_decay': 0.00037752261953927766}. Best is trial 1 with value: 0.8475750577367206.\n",
      "[I 2024-08-29 14:21:04,391] Trial 2 finished with value: 0.8452655889145496 and parameters: {'hidden_channels': 31, 'learning_rate': 0.01741645048055879, 'weight_decay': 0.0015319354147326379}. Best is trial 1 with value: 0.8475750577367206.\n",
      "[I 2024-08-29 14:21:06,740] Trial 3 finished with value: 0.8383371824480369 and parameters: {'hidden_channels': 19, 'learning_rate': 0.04102617098461099, 'weight_decay': 0.0037236391648030106}. Best is trial 1 with value: 0.8475750577367206.\n",
      "[I 2024-08-29 14:21:09,017] Trial 4 finished with value: 0.6235565819861432 and parameters: {'hidden_channels': 17, 'learning_rate': 0.08055918227002716, 'weight_decay': 0.0004130015271643662}. Best is trial 1 with value: 0.8475750577367206.\n",
      "[I 2024-08-29 14:21:09,046] Trial 5 pruned. \n",
      "[I 2024-08-29 14:21:09,082] Trial 6 pruned. \n",
      "[I 2024-08-29 14:21:12,175] Trial 7 finished with value: 0.8383371824480369 and parameters: {'hidden_channels': 31, 'learning_rate': 0.012698722727125633, 'weight_decay': 0.0005791740347300566}. Best is trial 1 with value: 0.8475750577367206.\n",
      "[I 2024-08-29 14:21:12,215] Trial 8 pruned. \n",
      "[I 2024-08-29 14:21:12,238] Trial 9 pruned. \n",
      "[I 2024-08-29 14:21:12,285] Trial 10 pruned. \n",
      "[I 2024-08-29 14:21:12,335] Trial 11 pruned. \n",
      "[I 2024-08-29 14:21:12,396] Trial 12 pruned. \n",
      "[I 2024-08-29 14:21:12,555] Trial 13 pruned. \n",
      "[I 2024-08-29 14:21:12,593] Trial 14 pruned. \n",
      "[I 2024-08-29 14:21:12,627] Trial 15 pruned. \n",
      "[I 2024-08-29 14:21:12,668] Trial 16 pruned. \n",
      "[I 2024-08-29 14:21:12,706] Trial 17 pruned. \n",
      "[I 2024-08-29 14:21:12,762] Trial 18 pruned. \n",
      "[I 2024-08-29 14:21:12,801] Trial 19 pruned. \n",
      "[I 2024-08-29 14:21:12,846] Trial 20 pruned. \n",
      "[I 2024-08-29 14:21:12,998] Trial 21 pruned. \n",
      "[I 2024-08-29 14:21:13,038] Trial 22 pruned. \n",
      "[I 2024-08-29 14:21:13,079] Trial 23 pruned. \n",
      "[I 2024-08-29 14:21:13,115] Trial 24 pruned. \n",
      "[I 2024-08-29 14:21:13,156] Trial 25 pruned. \n",
      "[I 2024-08-29 14:21:13,303] Trial 26 pruned. \n",
      "[I 2024-08-29 14:21:13,347] Trial 27 pruned. \n",
      "[I 2024-08-29 14:21:13,476] Trial 28 pruned. \n",
      "[I 2024-08-29 14:21:13,514] Trial 29 pruned. \n",
      "[I 2024-08-29 14:21:13,560] Trial 30 pruned. \n",
      "[I 2024-08-29 14:21:16,169] Trial 31 finished with value: 0.859122401847575 and parameters: {'hidden_channels': 17, 'learning_rate': 0.05917446282936153, 'weight_decay': 0.004473700532865897}. Best is trial 31 with value: 0.859122401847575.\n",
      "[I 2024-08-29 14:21:16,373] Trial 32 pruned. \n",
      "[I 2024-08-29 14:21:16,412] Trial 33 pruned. \n",
      "[I 2024-08-29 14:21:16,449] Trial 34 pruned. \n",
      "[I 2024-08-29 14:21:16,499] Trial 35 pruned. \n",
      "[I 2024-08-29 14:21:16,633] Trial 36 pruned. \n",
      "[I 2024-08-29 14:21:17,053] Trial 37 pruned. \n",
      "[I 2024-08-29 14:21:17,094] Trial 38 pruned. \n",
      "[I 2024-08-29 14:21:17,133] Trial 39 pruned. \n",
      "[I 2024-08-29 14:21:17,174] Trial 40 pruned. \n",
      "[I 2024-08-29 14:21:17,300] Trial 41 pruned. \n",
      "[I 2024-08-29 14:21:17,336] Trial 42 pruned. \n",
      "[I 2024-08-29 14:21:17,374] Trial 43 pruned. \n",
      "[I 2024-08-29 14:21:17,490] Trial 44 pruned. \n",
      "[I 2024-08-29 14:21:17,604] Trial 45 pruned. \n",
      "[I 2024-08-29 14:21:17,644] Trial 46 pruned. \n",
      "[I 2024-08-29 14:21:17,768] Trial 47 pruned. \n",
      "[I 2024-08-29 14:21:17,802] Trial 48 pruned. \n",
      "[I 2024-08-29 14:21:20,867] Trial 49 finished with value: 0.8406466512702079 and parameters: {'hidden_channels': 34, 'learning_rate': 0.03410704791689153, 'weight_decay': 0.0004301353771897861}. Best is trial 31 with value: 0.859122401847575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.859122401847575\n",
      "  Params: \n",
      "    hidden_channels: 17\n",
      "    learning_rate: 0.05917446282936153\n",
      "    weight_decay: 0.004473700532865897\n",
      "Epoch: 010, Loss: 1.9140, Train Acc: 0.2963, Train MSE: 0.0922, Val Acc: 0.3303, Val MSE: 0.0862\n",
      "Epoch: 020, Loss: 1.9273, Train Acc: 0.2963, Train MSE: 0.0908, Val Acc: 0.3303, Val MSE: 0.0879\n",
      "Epoch: 030, Loss: 1.7397, Train Acc: 0.2963, Train MSE: 0.0971, Val Acc: 0.3303, Val MSE: 0.0890\n",
      "Epoch: 040, Loss: 1.3591, Train Acc: 0.5170, Train MSE: 0.0904, Val Acc: 0.5150, Val MSE: 0.0889\n",
      "Epoch: 050, Loss: 0.9995, Train Acc: 0.8071, Train MSE: 0.0882, Val Acc: 0.7182, Val MSE: 0.0864\n",
      "Epoch: 060, Loss: 0.6421, Train Acc: 0.8858, Train MSE: 0.0837, Val Acc: 0.7714, Val MSE: 0.0781\n",
      "Epoch: 070, Loss: 0.5713, Train Acc: 0.8981, Train MSE: 0.0846, Val Acc: 0.7852, Val MSE: 0.0797\n",
      "Epoch: 080, Loss: 0.5394, Train Acc: 0.9213, Train MSE: 0.0832, Val Acc: 0.8222, Val MSE: 0.0793\n",
      "Epoch: 090, Loss: 0.5178, Train Acc: 0.9244, Train MSE: 0.0837, Val Acc: 0.8291, Val MSE: 0.0803\n",
      "Epoch: 100, Loss: 0.5221, Train Acc: 0.9290, Train MSE: 0.0835, Val Acc: 0.8176, Val MSE: 0.0798\n",
      "Epoch: 110, Loss: 0.4923, Train Acc: 0.9252, Train MSE: 0.0833, Val Acc: 0.8245, Val MSE: 0.0789\n",
      "Epoch: 120, Loss: 0.4992, Train Acc: 0.9174, Train MSE: 0.0833, Val Acc: 0.8245, Val MSE: 0.0785\n",
      "Epoch: 130, Loss: 0.4787, Train Acc: 0.9275, Train MSE: 0.0832, Val Acc: 0.8129, Val MSE: 0.0793\n",
      "Epoch: 140, Loss: 0.5210, Train Acc: 0.9313, Train MSE: 0.0834, Val Acc: 0.8222, Val MSE: 0.0787\n",
      "Epoch: 150, Loss: 0.5119, Train Acc: 0.9306, Train MSE: 0.0831, Val Acc: 0.8176, Val MSE: 0.0788\n",
      "Epoch: 160, Loss: 0.4744, Train Acc: 0.9290, Train MSE: 0.0833, Val Acc: 0.8245, Val MSE: 0.0795\n",
      "Epoch: 170, Loss: 0.5045, Train Acc: 0.9290, Train MSE: 0.0834, Val Acc: 0.8152, Val MSE: 0.0797\n",
      "Epoch: 180, Loss: 0.4898, Train Acc: 0.9221, Train MSE: 0.0833, Val Acc: 0.8360, Val MSE: 0.0798\n",
      "Epoch: 190, Loss: 0.4911, Train Acc: 0.9290, Train MSE: 0.0834, Val Acc: 0.8199, Val MSE: 0.0798\n",
      "Epoch: 200, Loss: 0.4817, Train Acc: 0.9267, Train MSE: 0.0833, Val Acc: 0.8291, Val MSE: 0.0794\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(f'  Value: {trial.value}')\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "#The final model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "final_model = GCN(data.num_features, dataset.num_classes, best_params['hidden_channels'])\n",
    "final_optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train(final_model, final_optimizer, data)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        train_acc, train_mse = evaluate(final_model, data, data.train_mask)\n",
    "        val_acc, val_mse = evaluate(final_model, data, data.val_mask)\n",
    "        print(f'Epoch: {epoch+1:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Train MSE: {train_mse:.4f}, Val Acc: {val_acc:.4f}, Val MSE: {val_mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1ee503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8291, Test MSE: 0.0718\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "test_acc, test_mse = evaluate(final_model, data, data.test_mask)\n",
    "print(f'Test Accuracy: {test_acc:.4f}, Test MSE: {test_mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b9218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
